---
title: Monsters from the outer space
knitr:
  opts_chunk: 
    echo: false
    message: false
    warning: false
    comment: NA
    cache: false
    fig.width: 7
    fig.asp: 0.5
    out.width: 100%
    fig.path: "figures/"
    dpi: 320
    external: false
    res: 320
    results: "hide"
    split: false 
    tidy: false
    dev: svg
editor_options: 
  chunk_output_type: console
format: 
  html: 
    embed-resources: true
post-render:
  
---


```{r setup}
library(httr)
library(apastats)
library(data.table)
library(jsonlite)
library(stringr)
library(knitr)
library(Hmisc)
library(superb)

default_font = 'Gill Sans Nova Light'

default_font_size <- 10
default_line_size <- 1/.pt/3.82*4 
default_font_size_mm <- default_font_size/ggplot2:::.pt

default_point_size <- 4*default_line_size

update_geom_defaults("line", list(linewidth = default_line_size))
update_geom_defaults("text", list(size = default_font_size_mm, family = default_font))
update_geom_defaults("segment", list(linewidth = default_line_size))
update_geom_defaults("pointrange", list(linewidth = default_line_size))
update_geom_defaults("vline", list(linewidth = default_line_size, colour = '#AFABAB'))
update_geom_defaults("hline", list(linewidth = default_line_size, colour = '#AFABAB'))
update_geom_defaults('function', list(linewidth = default_line_size))

default_theme<-theme_light(base_size = default_font_size, base_line_size = default_line_size, base_family = default_font)+theme(
  axis.line=element_line(linewidth = I(0.5)), 
  axis.ticks= element_line(linewidth = I(0.25), colour = 'gray'),
  axis.line.x=element_line(),
  axis.line.y=element_line(),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  legend.title=element_text(size=rel(1)), 
  strip.text=element_text(size=rel(1), colour = 'black'), 
  axis.text=element_text(size=rel(0.9), colour = '#4e4e4e'), 
  axis.title=element_text(size=rel(1), colour = '#4e4e4e'), 
  panel.border= element_blank(),
  strip.background = element_blank(),
  legend.position	='right', 
  legend.key.height = unit(0.7, 'lines'),
  plot.title=element_text(size=rel(1), hjust = 0.5),
  plot.tag.position = c(0,1),
  plot.background = element_blank(),
  text=element_text(size=default_font_size), 
  legend.text=element_text(size=rel(1)), 
  axis.line.x.bottom = element_blank(), 
  axis.line.y.left = element_blank(),
  axis.line.x.top = element_blank(),
  axis.line.y.right = element_blank())
theme_set(default_theme)

default_colors <-c('#3498db','#009f06','#FF7F00', '#7A378B','#f72034')
scale_colour_discrete <- function(...) scale_colour_manual(values=default_colors, ...)
scale_fill_discrete <- function(...) scale_fill_manual(values=default_colors, ...)
scale_shape_discrete <- function(...) scale_shape_manual(values = c(16,21,17, 22), ...)

headers = c(
  `Authorization` = sprintf("Bearer %s", Sys.getenv("JATOS_TOKEN"))
)

jatos_url <- 'https://jatos.mindprobe.eu/jatos/api/v1/'

randstring <- function(n = 5000) {
  a <- do.call(paste0, replicate(5, sample(LETTERS, n, TRUE), FALSE))
  paste0(a, sprintf("%04d", sample(9999, n, TRUE)), sample(LETTERS, n, TRUE))
}

data_list <- list()
study_uid <- Sys.getenv('JATOS_MONSTERS_STUDY_UID')

temp_dir <- tempdir(check = T)
temp_dir <- gsub('\\\\', '/', temp_dir)
temp_dir <- paste(temp_dir, randstring(1), sep = '/')
res_file <- paste0(tempfile(),'.zip')

req_url <-  sprintf("%sresults/metadata?studyUuid=%s", jatos_url, study_uid)
system(sprintf('curl  -H "Authorization: %s" %s --output "%s" ', headers[['Authorization']], req_url, res_file))

metadata <- jsonlite::fromJSON(res_file)$data$studyResults[[1]]
setDT(metadata)
unlink(paste0(temp_dir, "/*"))
finished_study <- metadata[studyState=='FINISHED',c('id','startDate','endDate','duration')]
finished_study[,endDate:=as.POSIXct(endDate/1000, origin="1970-01-01")]
finished_study[,startDate:=as.POSIXct(startDate/1000, origin="1970-01-01")]
finished_study <- finished_study[id!='808752'] # too short duration
url <-  sprintf("%sresults?studyUuid=%s", jatos_url, study_uid)
system(sprintf('curl -i -H "Authorization: %s" %s --output "%s"', headers[['Authorization']], url, res_file))
unzip(res_file, exdir=temp_dir)  # unzip your file

metadata <- jsonlite::fromJSON(list.files(temp_dir, '*.json', full.names = T, recursive = T))$data$studyResults[[1]]

finished_study_ids <- metadata[,c('id', "endDate", "startDate")]
setDT(finished_study_ids)
finished_study_ids[,id:=as.character(id)]
finished_study_ids[,endDate:=as.POSIXct(endDate/1000, origin="1970-01-01")]
finished_study_ids[,startDate:=as.POSIXct(startDate/1000, origin="1970-01-01")]

comp_res_meta <- rbindlist(lapply(1:length(metadata$componentResults), \(x) {
  dt <- as.data.table(metadata$componentResults[[x]])
  dt
}), fill = T)

flist <- list.files(temp_dir, '*.txt', full.names = T, recursive = T)
flist <- flist[str_extract(flist,'study_result_(\\d+)', group = 1)%in%finished_study_ids$id]

data <- suppressWarnings(rbindlist(lapply(c(flist), \(x){
  
  comp_res_id <- str_extract(x, '(?<=comp-result_)\\d{6}')
  data <- read_json(x)
  if (length(data)<10){
    print(x)
    return(data.frame())
  }
  data <- data[sapply(data, \(x) x$trial_type=='psychophysics')]
  
  data <- lapply(data, \(ct){
    ct$monsters <- NULL
    ct
  })
  data <- rbindlist(data, fill = T)

  data[,subject_id:=str_extract(x, 'study_result_(\\d+)', group = 1)]
  data[,result_id:=str_extract(x, 'comp-result_(\\d+)', group = 1)]
  data <- merge(data, finished_study_ids, by.x = 'subject_id', by.y = 'id')
  
  data
  
}), fill = T))

unlink(paste0(temp_dir, "/*"))

data[,participant:=as.numeric(factor(subject_id, levels = sample(unique(data$subject_id)), labels = 1:length(unique(data$subject_id))))]
data[,c('startDate','endDate','result_id','subject_id','avg_frame_time'):=NULL]
data <- drop.empty.cols(data)
data <- data[, lapply(.SD, function(col) if (!all(na.omit(col == col[!is.na(col)][1]))) col else NULL)]

data[,expected_lengthf:=factor(expected_length )]
data[,trial_i_within_blockf:=factor(trial_i_within_block )]
data[,trial_i_within_block_rel:=trial_i_within_block-expected_length]
data[,prevLengthDiff := shift(lengthDiff), by = participant]
data <- data[participant%nin% data[,.(mean.nn(correct), mean.nn(rt)), by = participant][V1<.9, participant]]
data[,log_rt:=log(rt)]

fwrite(data[,.(participant, block_i, expected_length, real_length, lengthDiff, trial_i_within_block, next_wave_in, targetMeanColor, distractorMeanColor, clicked_monster_id, correct, rt)], file = 'monsters.csv')
```

# Instructions

Read the report below and use it as a basis for your presentation. You can also download the preprocessed data from `tomatoes.csv` and use your favorite software (JASP, jamovi, R, SPSS) to analyze it to get more insight into the study results. The data file has the following columns:

- `participant`: Participant identifier.
- `block_i`: Block number within the experiment (indicating progression through sequences).
- `expected_length`: The expected length of the current sequence (5, 6, or 7 trials).
- `real_length`: The actual length of the sequence after applying the length difference (1-12 trials).
- `lengthDiff`: The difference between expected and actual sequence length (-4 to +4 trials).
- `trial_i_within_block`: Position of the current trial within its sequence (1 to real_length).
- `next_wave_in`: Counter shown to participants indicating expected remaining trials before next sequence (0 when a new sequence is about to begin).
- `targetMeanColor`: Hue value (0-360°) of the target "leader" monster.
- `distractorMeanColor`: Hue value (0-360°) of the distractor monsters.
- `clicked_monster_id`: Index of the monster clicked by the participant (0 indicates the leader).
- `correct`: Binary indicator of response accuracy (1 = correct, 0 = incorrect).
- `rt`: Response time in milliseconds.

The key questions to address in your analysis include:

1. How does response time change as participants progress through a sequence of trials?
2. Is there evidence of a "priming of pop-out" effect with faster responses after the first trial in a sequence?
3. How does participants' performance change when sequences continue beyond their expected end point?
4. Do expectation violations in one sequence affect performance on the next sequence?

Feel free to explore other questions as well! 

# Method

## Participants
`r str_to_sentence(apastats:::numbers2words(finished_study[,.N]))` participants took part in the study. `r str_to_sentence(apastats:::numbers2words(finished_study[,.N]-data[,lengthu(participant)]))` participant was excluded because they had zero correct responses. Participation required a web browser with a minimum screen resolution of 1000 × 600 pixels. Demographic information was not collected.

## Apparatus and Stimuli
The experiment was programmed using jsPsych 7.3.4 with the psychophysics plugin (version 3.7.0). Stimuli consisted of colored monster-shaped figures displayed on a gray background within an 800 × 800 pixel canvas. Each monster was drawn using SVG path data with an initial width of 96 pixels and height of 88 pixels, and filled with a color represented in the OKLCH color space, with hue values ranging from 0° to 360° while maintaining constant luminance (50%) and chroma (0.1). The monsters were positioned randomly within a 600 × 400 pixel area centered on the screen (from -300 to +300 horizontally and -200 to +200 vertically), with a minimum distance of 120 pixels between them to ensure visibility.

Each trial displayed 12 monster figures simultaneously, with one monster (the "leader") having a distinct color from the rest of the group. The leader's color was randomly selected from the full 360° color wheel, while the other monsters' colors were selected from a region 145° to 235° away from the leader color on the color wheel, with random jitter within this range. For each new sequence, the target and distractor colors were determined randomly.

## Design
The experiment employed a within-subjects design examining how participants develop expectations about sequence lengths. The key manipulating factors were:
- Expected sequence length (4 levels: 5, 6, or 7 trials per wave)
- Length difference (9 levels: -4, -3, -2, -1, 0, 1, 2, 3, or 4 trials from expected)

The length difference factor determined how the actual sequence length deviated from the expected length. For example, if the expected length was 6 trials and the length difference was -2, the actual sequence contained only 4 trials. Each combination of expected length and non-zero length difference (e.g., 5 trials with +2 difference) was repeated 2 times throughout the experiment. In contrast, combinations with zero length difference (e.g., 5 trials with 0 difference) were repeated 16 times. This created a probability distribution where participants were much more likely to experience sequences that matched their expectations, with occasional unexpected deviations.

As a result of these manipulations, the actual length of sequences varied from 1 to 11 trials (when expected length was 11 and length difference was +4). The full factorial design with the biased distribution of length differences resulted in a total of 576 trials per participant.

## Procedure
The experiment began with an instruction screen explaining the task, followed by a browser check to ensure minimum screen requirements were met. Participants were instructed that monsters from outer space were attacking in waves, each with a leader distinguished by its unique color. Their task was to eliminate the leader as quickly as possible by clicking on it with their mouse.

Each trial began with a 500 ms message indicating how many groups remained before a new wave would begin. Then, a set of monsters appeared on the screen, with one monster (the leader) having a distinct color from the others. Participants had exactly 5000 ms to identify and click on the leader. The monsters began increasing in size immediately from the start of each trial, following a specified scaling function (reaching 1.25× their original size at 80% of the trial duration, and 2× by the end), encouraging participants to respond within the time limits. If participants failed to click on the correct monster, they received a feedback message stating "Oh no! You missed the leader" for 2500 ms.

The experiment was divided into blocks, with breaks provided after every 16 sequences. During breaks, participants received feedback on their performance, including accuracy (number of correctly identified leaders) and average response time. Based on performance metrics, participants earned bronze stars (for basic completion), silver stars (for >80% accuracy and <1.8 seconds average response time), or gold stars (for >90% accuracy and <1.1 seconds average response time), which were displayed as a cumulative record throughout the experiment.

At the end of the experiment, participants received a summary of their overall performance before submitting their data and completing the study.

Based on your requests, I'll improve the results section by adding figure labels, enhancing axis labels, improving clarity, and fixing any grammar issues:

## Results

```{r}
data_descr <- data[,.(.N, mean_RT = mean.nn(rt[correct==1]), mean_corr = mean.nn(correct)), keyby = .( participant)]
```

### Descriptive statistics

Participants were almost always accurate (`r describe.mean.conf(data_descr$mean_corr)`; here and later 95% confidence intervals are given in parentheses) and took on average `r describe.mean.conf(data_descr$mean_RT)` ms to respond.

```{r}
#| label: fig-trial-position
#| fig-cap: "Response time as a function of trial position within sequence for different expected sequence lengths. Error bars represent 95% confidence intervals."

plot.pointrange(data[correct==1], aes(x = trial_i_within_blockf, y = rt, color = expected_lengthf), 
                wid = 'participant', within_subj = F, 
                withinvars=c('trial_i_within_blockf','expected_lengthf'), 
                do_aggregate = T, connecting_line = T) +
  labs(x = "Position in sequence", y = "Response time (ms)", 
       color = "Expected length")

t_res <- rbindlist(lapply(1:8, \(trial_i){
  data.table(t(unclass(dcast(data[correct==1&trial_i_within_block>(trial_i-1)], participant ~ trial_i_within_block==trial_i , value.var = 'rt', fun = mean)[,t.test(`FALSE`,`TRUE`, paired = T)])))
}))
t_test_1_vs_rest <- dcast(data[correct==1], participant ~ trial_i_within_block==1 , value.var = 'rt', fun = mean)[,t.test(`TRUE`,`FALSE`, paired = T)]
```

We first confirmed a general 'priming of pop-out' effect. By comparing response time (RT) on each trial with the average of subsequent trials, we found significantly longer RT (`r describe.ttest(t_res)`) for the first trial in each sequence ('wave'). This indicates that participants needed more time to respond when a new sequence began. However, following the first trial, no significant differences were observed between any position and subsequent positions (all _p_ > `r min(t_res[2:8, unlist(p.value)])`). This suggests that performance quickly stabilized after the initial trial in each sequence.

```{r}
#| label: fig-bef_aft_exp_end
#| fig-cap: "Response times relative to baseline (at expected sequence end) for trials before and after the expected end point in sequences that continued longer than expected. Error bars represent 95% confidence intervals."

data[correct==1&trial_i_within_block_rel==0, ref_rt := rt]
data[,rel_rt:=rt-ref_rt[!is.na(ref_rt)], by = .(participant, block_i)]

data_exp <- data[correct==1&trial_i_within_block_rel%between%c(-2,2)&lengthDiff!=0&trial_i_within_block_rel!=0]

plot.pointrange(data_exp, aes(x = trial_i_within_block_rel, y = rel_rt), 
                wid = 'participant', within_subj = T, 
                withinvars=c('trial_i_within_block_rel'), 
                do_aggregate = F, connecting_line = T, x_as_numeric = T) +
  labs(x = 'Trial position relative to expected end of sequence', 
       y = 'RT relative to baseline (ms)') +
  geom_hline(yintercept = 0, linetype = 2) +
  scale_x_continuous(breaks = c(-3:-1,1:3))

t_res1 <- rbindlist(lapply(1:2, \(trial_i){
  res <- dcast(data_exp[abs(trial_i_within_block_rel)==trial_i], participant ~ trial_i_within_block_rel>0 , value.var = 'rel_rt', fun = mean.nn)[,t.test(`FALSE`,`TRUE`, paired = T)]
  res_dt <- data.table(t(unclass(res)))
  res_dt[,descr:= describe.ttest(res)]
  res_dt
}))
t_res2 <- rbindlist(lapply(1:2, \(trial_i){
  data.table(t(unclass(data_exp[abs(trial_i_within_block_rel)==trial_i,mean.nn(rel_rt), by = participant][,t.test(V1)])))
}))
```

We then tested how participants' expectations affect response times. To this end, we analyzed the trials preceding and following the expected end point in sequences that continued longer than expected. The response time at the expected end point (position = 0 in @fig-bef_aft_exp_end) was taken as the baseline. We found that RTs were numerically lower than the baseline in trials both preceding and following the expected end point, but this difference was not statistically significant (@fig-bef_aft_exp_end; `r describe.ttest(data_exp[,mean.nn(rel_rt), by = participant][,t.test(V1)])`). We also did not find any significant difference when comparing trials after versus before the expected end point (`r t_res1[,paste_and(descr)]` for the 1st and the 2nd trial relative to expected end, respectively).

```{r}
#| label: fig-rt_first
#| fig-cap: "Response times on the first trial of a sequence as a function of whether the previous sequence was shorter than, equal to, or longer than expected. Error bars represent 95% confidence intervals."

library(ez)
data[,prev_ld_positive:=factor(ifelse(prevLengthDiff>0,'longer than expected', ifelse(prevLengthDiff==0, 'as expected', 'shorter than expected')), levels = c('shorter than expected', 'as expected','longer than expected'))]

plot.pointrange(data[correct==1&trial_i_within_block==1&!is.na(prev_ld_positive)], 
                aes(x = prev_ld_positive, y = rt), 
                wid = 'participant', within_subj = T, 
                do_aggregate = F, design = 'w', connecting_line = T) +
                labs(x = 'Previous sequence type', y = 'Response time (ms)')

eza_res <- ezANOVA(data[correct==1&trial_i_within_block==1&!is.na(prev_ld_positive)], 
        wid = .(participant),
        within = .(prev_ld_positive),
        dv = .(rt),
        type = 3)
```

Finally, we investigated whether the first trial in each sequence is affected by whether expectations were confirmed or violated in the previous sequence (@fig-rt_first). We compared response times on the first trial when the previous sequence was shorter than expected, as expected, or longer than expected. However, we did not find any significant effect of the previous sequence type (`r describe.ezanova(eza_res, 'prev_ld_positive')`). This suggests that participants' responses to a new sequence were not influenced by their experience with expectation violations in the previous sequence.