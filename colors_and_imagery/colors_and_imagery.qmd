---
title: Color memory and imagery
knitr:
  opts_chunk: 
    echo: false
    message: false
    warning: false
    comment: NA
    cache: false
    fig.width: 7
    fig.asp: 0.5
    fig.path: "figures_colors/"
    dpi: 320
    external: false
    res: 320
    out.width: 100%
    results: "hide"
    split: false 
    tidy: false
    dev: svg
editor_options: 
  chunk_output_type: console
format: 
  html: 
    embed-resources: true
---


```{r setup}
library(httr)
library(apastats)
library(data.table)
library(jsonlite)
library(stringr)
library(ez)
library(knitr)
library(Hmisc)
library(superb)
library(circhelp)
library(patchwork)

fig_width <- 7
default_font = 'Gill Sans Nova Light'

default_font_size <- 10
default_line_size <- 1/.pt/3.82*4 
default_font_size_mm <- default_font_size/ggplot2:::.pt

default_point_size <- 4*default_line_size

update_geom_defaults("line", list(linewidth = default_line_size))
update_geom_defaults("text", list(size = default_font_size_mm, family = default_font))
update_geom_defaults("segment", list(linewidth = default_line_size))
update_geom_defaults("pointrange", list(linewidth = default_line_size))
update_geom_defaults("vline", list(linewidth = default_line_size, colour = '#AFABAB'))
update_geom_defaults("hline", list(linewidth = default_line_size, colour = '#AFABAB'))
update_geom_defaults('function', list(linewidth = default_line_size))

default_theme<-theme_light(base_size = default_font_size, base_line_size = default_line_size, base_family = default_font)+theme(
  axis.line=element_line(linewidth = I(0.5)), 
  axis.ticks= element_line(linewidth = I(0.25), colour = 'gray'),
  axis.line.x=element_line(),
  axis.line.y=element_line(),
  panel.grid.major = element_blank(), 
  panel.grid.minor = element_blank(),
  legend.title=element_text(size=rel(1)), 
  strip.text=element_text(size=rel(1), colour = 'black'), 
  axis.text=element_text(size=rel(0.9), colour = '#4e4e4e'), 
  axis.title=element_text(size=rel(1), colour = '#4e4e4e'), 
  panel.border= element_blank(),
  strip.background = element_blank(),
  legend.position	='right', 
  legend.key.height = unit(0.7, 'lines'),
  plot.title=element_text(size=rel(1), hjust = 0.5),
  plot.tag.position = c(0,1),
  plot.background = element_blank(),
  text=element_text(size=default_font_size), 
  legend.text=element_text(size=rel(1)), 
  axis.line.x.bottom = element_blank(), 
  axis.line.y.left = element_blank(),
  axis.line.x.top = element_blank(),
  axis.line.y.right = element_blank())
theme_set(default_theme)

default_colors <-c('#3498db','#009f06','#FF7F00', '#7A378B','#f72034')
scale_colour_discrete <- function(...) scale_colour_manual(values=default_colors, ...)
scale_fill_discrete <- function(...) scale_fill_manual(values=default_colors, ...)
scale_shape_discrete <- function(...) scale_shape_manual(values = c(16,21,17, 22), ...)



gg_crosshair <- function(xint = 0, yint = 0, lty = 2) {
  if (!is.null(xint)) gv = geom_vline(xintercept = xint, linetype = lty)
  else gv = NULL
  if (!is.null(yint)) gh = geom_hline(yintercept = yint, linetype = lty)
  else gh = NULL
  
  list(gv, gh)
}

headers = c(
  `Authorization` = sprintf("Bearer %s", Sys.getenv("JATOS_TOKEN"))
)

jatos_url <- 'https://jatos.mindprobe.eu/jatos/api/v1/'

randstring <- function(n = 5000) {
  a <- do.call(paste0, replicate(5, sample(LETTERS, n, TRUE), FALSE))
  paste0(a, sprintf("%04d", sample(9999, n, TRUE)), sample(LETTERS, n, TRUE))
}

data_list <- list()
study_uid <- Sys.getenv('JATOS_COLORS_AND_IMAGERY_STUDY_UID')

temp_dir <- tempdir(check = T)
temp_dir <- gsub('\\\\', '/', temp_dir)
temp_dir <- paste(temp_dir, randstring(1), sep = '/')
res_file <- paste0(tempfile(),'.zip')

req_url <-  sprintf("%sresults/metadata?studyUuid=%s", jatos_url, study_uid)
system(sprintf('curl  -H "Authorization: %s" %s --output "%s" ', headers[['Authorization']], req_url, res_file))

metadata <- jsonlite::fromJSON(res_file)$data$studyResults[[1]]
setDT(metadata)
unlink(paste0(temp_dir, "/*"))
finished_study <- metadata[studyState=='FINISHED',c('id','startDate','endDate','duration')]


url <-  sprintf("%sresults?studyUuid=%s", jatos_url, study_uid)
system(sprintf('curl -i -H "Authorization: %s" %s --output "%s"', headers[['Authorization']], url, res_file))
unzip(res_file, exdir=temp_dir)  # unzip your file

metadata <- jsonlite::fromJSON(list.files(temp_dir, '*.json', full.names = T, recursive = T))$data$studyResults[[1]]

finished_study_ids <- metadata[,c('id', "endDate", "startDate")]
setDT(finished_study_ids)
finished_study_ids[,id:=as.character(id)]
finished_study_ids[,endDate:=as.POSIXct(endDate/1000, origin="1970-01-01")]
finished_study_ids[,startDate:=as.POSIXct(startDate/1000, origin="1970-01-01")]
finished_study_ids <- finished_study_ids[!is.na(endDate)] 
finished_study_ids <- finished_study_ids[startDate>Sys.getenv("JATOS_START_DATE")]

comp_res_meta <- rbindlist(lapply(1:length(metadata$componentResults), \(x) {
  dt <- as.data.table(metadata$componentResults[[x]])
  dt
}), fill = T)

flist <- list.files(temp_dir, '*.txt', full.names = T, recursive = T)
flist <- flist[str_extract(flist,'study_result_(\\d+)', group = 1)%in%finished_study_ids$id]
# on the next line, one id is manually excluded as this participant did the vviq part twice, so we use the latest result
flist_vviq <- flist[str_extract(flist,'comp-result_(\\d+)', group = 1)%in%comp_res_meta[componentId=='29868'&id!='1067177',id]]
flist_colors <- flist[str_extract(flist,'comp-result_(\\d+)', group = 1)%in%comp_res_meta[componentId!='29868',id]]


data_vviq <- rbindlist(lapply(c(flist_vviq), \(x){
  data <- read_json(x)
  data.table(participant = str_extract(x, 'study_result_(\\d+)', group = 1),
             result_id=str_extract(x, 'comp-result_(\\d+)', group = 1),
             vviq = sum(sapply(data,\(q) ifelse(length(q$response)==4, do.call(sum, q$response), 0))))
}))

data_color <- suppressWarnings(rbindlist(lapply(c(flist_colors), \(x){
  # print(x)
  comp_res_id <- str_extract(x, '(?<=comp-result_)\\d{6}')
  data <- read_json(x)
  if (length(data)<10){
    return(data.frame())
  }
  browser_check <- data.frame(data[[1]])
  setDT(browser_check)
  
  data <- data[sapply(data, \(x) x$trial_type=='psychophysics')]
  data <- rbindlist(data, fill = T)
  if ('score' %nin% colnames(data)){
    return(data.frame())
  }

    
  data[,score:=as.numeric(score)]
  data[!is.na(left_colors), left_sd:=circ_sd_360(as.numeric(str_split(left_colors,',', simplify = T))), by = left_colors]
  data[!is.na(right_colors), right_sd:=circ_sd_360(as.numeric(str_split(right_colors,',', simplify = T))), by = right_colors]
  data[,left_colors:=NULL]
  data[,right_colors:=NULL]
  setnafill(data, type = "locf",cols = c('target_color','distr_color','left_color','right_color','target_sd', 'distr_sd', 'target_x', 'distr_x', 'similarity_bin', 'color_bin','left_sd','right_sd','cue_pos','cued_color','cue_n'))
  setnafill(data, type = "nocb",cols = c('score'))
  #
  if (browser_check[1, .(trial_type)]=='browser-check'){
    data[,c('window_width','window_height'):=browser_check[,.(width, height)]]
  }
  data<-data[trial_type=='psychophysics'&seq_type=='response']

  data[,participant:=str_extract(x, 'study_result_(\\d+)', group = 1)]
  data[,result_id:=str_extract(x, 'comp-result_(\\d+)', group = 1)]
  data <- merge(data, finished_study_ids, by.x = 'participant', by.y = 'id')
  
  data
  
}), fill = T))

unlink(paste0(temp_dir, "/*"))

data_color <- data_color[participant%in%finished_study$id]
data_vviq <- data_vviq[participant%in%finished_study$id]
data_color <- merge(data_color, data_vviq, by = 'participant')
data_color[,participant:=paste0('S',as.numeric(factor(participant, levels = sample(unique(data_color$participant)), labels = 1:length(unique(data_color$participant)))))]

data_color[,c('startDate','endDate','result_id','avg_frame_time'):=NULL]

setnafill(data_color, type = "locf", cols = c('window_height','window_width'))
data_color <- drop.empty.cols(data_color)

data_color <- data_color[, lapply(.SD, function(col) if (!all(na.omit(col == col[!is.na(col)][1]))) col else NULL)]


setnames(data_color,c('target_sd','distr_sd'), c('target_noise','distr_noise'), skip_absent = T)
data_color[,abs_err:=abs(error)]

data_color[cue_pos!=target_x, c('target_noise','distr_noise'):=.(distr_noise, target_noise)]
data_color[cue_pos!=target_x, c('target_color','distr_color'):=.(distr_color, target_color)]
data_color[,target_noise:=factor(target_noise, levels = c(5, 20), labels = c('low','high'))]
data_color[,target_sample_sd:=ifelse(cue_pos<0, left_sd, right_sd)]
data_color[,distr_sample_sd:=ifelse(cue_pos>0, left_sd, right_sd)]
data_color[,distr_noise:=factor(distr_noise, levels = c(5, 20), labels = c('low','high'))]
data_color[,td_dist:=angle_diff_360(distr_color, target_color)]

data_color[,bias_distr:= sign(td_dist)*error]
data_color[,distr_color:=angle_diff_360(distr_color,0)]
data_color <- unique(data_color)
data_color[!is.na(error),n_by_subj:=.N, by = participant]
data_color[!is.na(error),n_by_block:=.N, by = .(block, participant)]

data_color[, abs_td_dist:=abs(td_dist)]
data_color[,target_color:=angle_diff_360(target_color, 0)]
data_color[,similarity_binf:=factor(similarity_bin)]
data_color[,noise:=interaction(target_noise, distr_noise, sep = ' - ')]
data_color[,.(totalN = .N, circ_sd = circ_sd_360(error, na.rm = T),n_by_subj[1], missed_responses = sum(is.na(error))), keyby = .(participant)]
data_color[,.(.N, circ_sd = circ_sd_360(error, na.rm = T), mean_abs_err= mean.nn(abs_err)), keyby = .(  participant)]

#outliers 
data_color<-data_color[participant%nin%data_color[,.(.N, circ_sd_360(error, na.rm = T)), by = participant][V2>60|N<500, participant]]


data_color[,.(.N, circ_sd_360(error, na.rm = T)), by = participant]
db_res <- lapply(split(data_color[!is.na(error),], by = c('participant', 'cue_n')), \(dt){
  dt[,c('be_c','is_outlier','rc_pred','rc_group','shifted_td_dist','mad_outlier'):=remove_cardinal_biases_discrete(error, similarity_bin, space = 360)[,c('be_c','is_outlier')], by = .(participant, cue_n)]
  dt
})

data_color <- rbindlist(db_res)
data_color[is_outlier==F&abs_err>90, is_outlier:=T]
data_color[,.(.N, circ_sd = circ_sd_360(error, na.rm = T),circ_sd_correct = circ_sd_360(error, na.rm = T), n_by_subj=n_by_subj[1], n_nonmissing=sum(!is.na(error)), share_outliers= mean.nn(is_outlier), mean_abs_err= mean.nn(abs_err)), keyby = .(participant)]

data_color[,bias_to_distr_corr:=sign(td_dist)*be_c]
data_color[,bias_to_distr_raw:=sign(td_dist)*error]

data_color[,abs_be_c:=abs(be_c)]
data_color[,avg_abs_err:=mean.nn(abs(be_c)), by = participant]
data_color[,noise_equal:=factor(distr_noise==target_noise, levels = c(F, T), labels = c('Unequal noise', 'Equal noise'))]
data_color[,bias_distr:=NULL]
data_color[,similarity_binf:=factor(similarity_bin)]

data_color <- data_color[is_outlier==F]
data_color <- data_color[, lapply(.SD, function(col) if (!all(na.omit(col == col[!is.na(col)][1]))) col else NULL)]

fwrite(data_color[,.(participant, vviq, block, trial_number, similarity_bin, target_color, distr_color, td_dist, left_color, right_color, target_noise, distr_noise, target_x, distr_x, cue_n, cue_pos, cued_color, resp_color,  error, abs_err, noise, be_c, abs_be_c,  bias_to_distr_raw, bias_to_distr_corr     )], 'colors_and_imagery.csv')

```

# Instructions

Read the report and use it as a basis for your presentation. You can also download the preprocessed data from `colors_and_imagery.csv` and use your favorite software (JASP, jamovi, R, SPSS) to analyze it to get more insight into the study results. The data file has the following columns:

* `participant` - participant identifier (factor with levels "S1", "S2", etc.)
* `vviq` - Visual Vividness Imagery Questionnaire score (higher scores indicate more vivid imagery)
* `block` - block number (0 = training, 1-2 = main experimental blocks)
* `trial_number` - trial number within the experiment
* `similarity_bin` - angular distance between target and distractor colors in degrees (20°, 45°, or 135°)
* `target_color` - the target color value in degrees (0-360°)
* `distr_color` - the distractor color value in degrees (0-360°)
* `td_dist` - difference between target and distractor colors in degrees
* `left_color` - color value of the left stimulus in degrees
* `right_color` - color value of the right stimulus in degrees
* `target_noise` - noise level of the target stimulus (factor: "low" or "high")
* `distr_noise` - noise level of the distractor stimulus (factor: "low" or "high")
* `target_x` - x-coordinate of the target position (-300 for left, 300 for right)
* `distr_x` - x-coordinate of the distractor position (-300 for left, 300 for right)
* `cue_n` - cue number (1 = first report, 2 = second report)
* `cue_pos` - position of the cue (-300 for left, 300 for right)
* `cued_color` - the color value that was cued (the one to be reported) in degrees
* `resp_color` - the color value reported by the participant in degrees
* `error` - angular deviation between the target color and response color in degrees
* `abs_err` - absolute value of the error in degrees
* `noise` - combined noise condition from target and distractor (factor with 4 levels: "low - low", "high - low", "low - high", "high - high")
* `be_c` - error after correction for individual biases (more reliable measure than raw error)
* `abs_be_c` - absolute value of the bias-corrected error
* `bias_to_distr_raw` - raw bias toward the distractor in degrees
* `bias_to_distr_corr` - bias-corrected bias toward the distractor in degrees

The study investigates the relationship between visual imagery ability (measured by VVIQ) and color memory performance. In particular, it examines how noise in the stimulus affects memory accuracy and whether this is moderated by individual differences in imagery ability. The key research questions are:

1. How does noise level in the target and distractor stimuli affect color memory accuracy?
2. Does imagery ability (VVIQ score) predict overall color memory performance?
3. Does imagery ability predict how strongly performance is affected by noise?
4. Does imagery ability predict how much performance degrades when reporting the second item compared to the first?

Feel free to explore other questions as well! 

# Method

## Participants
`r str_to_sentence(apastats:::numbers2words(finished_study[,.N]))` participants took part in the study. `r str_to_sentence(apastats:::numbers2words(finished_study[,.N]-data_color[,lengthu(participant)]))` participant was excluded because of very poor performance (an average absolute error was above the predefined threshold of 60 deg.).

## Procedure
Participants first completed a browser check to ensure their screen met the minimum requirements (1000 × 600 pixels) and were instructed to switch to fullscreen mode. They then received general instructions explaining the purpose of the study and were informed that they would complete a questionnaire followed by a memory task. Participants completed the Visual Vividness Imagery Questionnaire, after which they received detailed instructions for the color memory task. They then completed training on the color memory task followed by two experimental blocks with a short break halfway through each block.

## Materials
The experiment was implemented using jsPsych 7.3.4, a JavaScript library for conducting behavioral experiments in a web browser.

### Visual Vividness Imagery Questionnaire (VVIQ)
Participants completed a shortened version of the Visual Vividness Imagery Questionnaire (Marks, 1973), which assessed their self-reported ability to form mental images. The VVIQ included four scenarios (out of the original eight): visualizing a relative or friend, a sunrise, a shop front, and a countryside scene, presented in random order. For each scenario, participants were presented with a screen containing all four specific visual details to rate simultaneously. Participants rated the vividness of each detail on a 5-point scale, from "No image at all, you only 'know' that you are thinking of the object" (1) to "Perfectly clear and as vivid as real seeing" (5). Higher scores indicated more vivid visual imagery.

### Color Memory Task
Participants performed a color memory task that required them to remember the colors of two simultaneously presented colored patches.

#### Stimuli
Each colored patch consisted of an 8 × 8 grid of colored squares (64 squares total). Colors were represented in the OKLCH color space, with hue values ranging from 0° to 360°. The colors of individual squares within each patch were drawn from a normal distribution with a mean corresponding to the target color and a standard deviation that could be either 5° or 20° in hue angle. Target colors were selected from six color bins spaced evenly around the 360° color wheel (i.e., at approximately 60° intervals), with random jitter covering the entire bin to ensure the whole color space was sampled. The similarity between the two patches' mean colors was manipulated across three levels (20°, 45°, or 135° separation in hue angle). The patches were presented to the left and right of a central fixation point.

#### Trial Procedure
Each trial began with a central fixation circle (1000 ms), followed by the simultaneous presentation of two colored patches for a variable duration (1500 ms in the main blocks). After a brief memory delay (500 ms), a white circle (cue) appeared either to the left or right of fixation (500 ms), indicating which color patch the participant needed to report. Participants then saw a color wheel and were asked to select the color that best matched the average color of the cued patch by clicking on the appropriate position on the wheel. The color wheel had a random offset on each trial to prevent position-based response strategies. Participants had 5 seconds to respond. If they failed to respond within this time window, they received a "Too late! Try to respond faster next time" message for 2500 ms. After reporting the first color, participants were cued to report the color of the second patch following the same procedure.

#### Design
The experiment employed a 6 (color bin) × 2 (target position: left, right) × 2 (target standard deviation: 5°, 20°) × 2 (distractor standard deviation: 5°, 20°) × 3 (similarity bin: 20°, 45°, 135°) within-subjects factorial design.

#### Training
Before starting the main experimental trials, participants completed 10 training trials where stimulus presentation times were initially longer (4000 ms for the colored patches and 1000 ms for the cue) and gradually decreased to match the timing used in the main experiment (1500 ms and 500 ms, respectively). During training, participants received detailed feedback showing both their response and the correct answer after each trial.

Participants' performance was scored based on the angular deviation between their response and the target color, with higher scores awarded for more accurate responses.

# Results

## Vividness of Visual Imagery Questionnaire

```{r vviq-distribution}
#| label: fig-vviq
#| fig-cap: "Distribution of Visual Vividness Imagery Questionnaire (VVIQ) scores across participants. Higher scores indicate more vivid self-reported visual imagery."
#| fig-asp: 0.25

ggplot(data_vviq, aes(x = vviq)) +
  geom_dotplot(binwidth = 1) +
  coord_cartesian(xlim = c(20,80)) +
  labs(x = 'VVIQ score', y = 'Count') +
  theme(axis.title.y = element_blank(), 
        axis.text.y = element_blank(), 
        axis.ticks.y = element_blank())
```

The VVIQ scores were between `r min(data_vviq$vviq)` and `r max(data_vviq$vviq)` (`r describe.mean.conf(data_vviq$vviq)`).

## Color recall accuracy

```{r noise-effect}
#| label: fig-noise-effect
#| fig-cap: "Effect of target and distractor noise on color recall accuracy. Error bars represent 95% confidence intervals corrected for within-subject comparisons."
#| fig-asp: 0.75

plot.pointrange(data_color, aes(x = target_noise, 
                               y = abs_be_c, 
                               color = distr_noise),
               within_subj = TRUE, 
               wid = 'participant', 
               withinvars = c('target_noise','distr_noise')) +
  labs(x = 'Target noise level', 
       y = 'Average absolute error (degrees)',
       color = 'Distractor noise') +
  scale_color_discrete(labels = c("Low", "High")) +
  scale_x_discrete(labels = c("Low", "High")) 

# Run ANOVA
ez_res_abs_err <- ezANOVA(
  data = data_color,
  dv = .(abs_be_c),
  wid = .(participant),
  within = .(target_noise, distr_noise),
  type = 3
)
```

A repeated-measures ANOVA indicated a significant effect of target noise `r describe.ezanova(ez_res_abs_err, 'target_noise')` and a target × distractor noise interaction `r describe.ezanova(ez_res_abs_err, 'target_noise:distr_noise')`. Performance dropped significantly when the targets became more noisy: `r describe.ttest(dcast(data_color, participant~target_noise, value.var = 'abs_err', fun = mean)[,t.test(low, high, paired = T)])`. However, the effect of distractor noise was noticeable only when the target had a low noise level `r describe.ttest(dcast(data_color[target_noise=='low'], participant~distr_noise, value.var = 'abs_be_c', fun = mean)[,t.test(low, high, paired = T)])` but not when it had a high noise level `r describe.ttest(dcast(data_color[target_noise=='high'], participant~distr_noise, value.var = 'abs_be_c', fun = mean)[,t.test(low, high, paired = T)])`.

## Does imagery score predict the recall accuracy?

```{r vviq-correlations}
#| label: fig-vviq-corr
#| fig-cap: "Relationship between VVIQ scores and color memory performance. (A) Overall correlation between VVIQ and recall accuracy. (B) Correlation between VVIQ and the effect of target noise. (C) Correlation between VVIQ and the difference in performance between 2nd and 1st reported items."
#| fig-width: 10
#| fig-asp: 0.4

# Prepare data summaries
perf_vviq <- data_color[,.(mem_perf_bc = circ_sd_360(be_c), 
                           bias_to_distr = circ_mean_360(bias_to_distr_corr)), 
                        by = .(participant, vviq)]

perf_vviq_by_cue_n <- data_color[,.(mem_perf_bc = circ_sd_360(be_c), 
                                    bias_to_distr = circ_mean_360(bias_to_distr_corr)), 
                                 by = .(participant, vviq, cue_n)]

perf_vviq_by_noise <- data_color[,.(mem_perf_bc = circ_sd_360(be_c), 
                                    bias_to_distr = circ_mean_360(bias_to_distr_corr)), 
                                 by = .(participant, vviq, target_noise)]

# Create derived measures
delta_by_vviq <- dcast(perf_vviq_by_noise, 
                       participant+vviq~target_noise, 
                       value.var = 'mem_perf_bc')[,delta:=high-low]

delta_cue_n_by_vviq <- dcast(perf_vviq_by_cue_n, 
                            participant+vviq~cue_n, 
                            value.var = 'mem_perf_bc')[,delta:=`2`-`1`]

# Common theme for all plots

# Plot 1: Overall correlation between VVIQ and recall accuracy
p1 <- ggplot(perf_vviq, aes(x = vviq, y = mem_perf_bc)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  labs(x = "VVIQ score", y = "Memory performance\n(SD of errors)") 
  

# Plot 2: Correlation between VVIQ and noise effect
p2 <- ggplot(delta_by_vviq, aes(x = vviq, y = delta)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  labs(x = "VVIQ score", y = "Effect of noise\n(High - Low)") 
# Plot 3: Correlation between VVIQ and delta between 1st and 2nd item performance
p3 <- ggplot(delta_cue_n_by_vviq, aes(x = vviq, y = delta)) +
  geom_point() +
  geom_smooth(method = 'lm') +
  labs(x = "VVIQ score", y = "Performance difference\n(2nd item - 1st item)") 

# Combine plots with patchwork and add tags
(p1 + p2 + p3) + 
  plot_annotation(tag_levels = 'A') &
  theme(plot.tag = element_text(face = "bold", size = 12))
```

Overall, there was no correlation between recall accuracy and VVIQ score (`r perf_vviq[,describe.r(cor.test(mem_perf_bc, vviq))]`). Furthermore, the drop in performance in the high noise condition compared to the low noise condition also did not correlate with VVIQ scores (`r delta_by_vviq[,describe.r(cor.test(delta, vviq))]`). Finally, we analyzed the difference in performance between the second item and the first item (where positive values indicate worse performance for the second item), but also did not see any correlation with VVIQ score (`r delta_cue_n_by_vviq[,describe.r(cor.test(delta, vviq))]`).